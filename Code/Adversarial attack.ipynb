{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Adversarial attack.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPVTZjAuwpCVUnn8zFJHvqh"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["%pylab inline\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.utils.data.dataloader as dataloader\n","import torch.optim as optim\n","\n","from torch.utils.data import TensorDataset\n","from torch.autograd import Variable\n","from torchvision import transforms\n","from torchvision.datasets import MNIST\n","import matplotlib.pyplot as plt\n","import os\n","from skimage import io, transform \n","import cv2 as cv\n","import sklearn\n","from sklearn import metrics\n","\n","\n","SEED = 1\n","\n","# CUDA?\n","cuda = torch.cuda.is_available()\n","\n","# For reproducibility\n","torch.manual_seed(SEED)\n","\n","if cuda:\n","    torch.cuda.manual_seed(SEED)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"631_OB-RhnVN","executionInfo":{"status":"ok","timestamp":1652383939096,"user_tz":-480,"elapsed":6800,"user":{"displayName":"Peilin Zhan","userId":"07435443963307107662"}},"outputId":"9ab63ebd-8f22-4015-92f6-c076036e7dee"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Populating the interactive namespace from numpy and matplotlib\n"]}]},{"cell_type":"code","source":["# mount drive\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"StOlvgPY1GkQ","executionInfo":{"status":"ok","timestamp":1652383960803,"user_tz":-480,"elapsed":21717,"user":{"displayName":"Peilin Zhan","userId":"07435443963307107662"}},"outputId":"7d217007-6d96-496e-d036-556842a1b219"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}]},{"cell_type":"code","source":["import os\n","os.chdir(\"/content/gdrive/My Drive\")\n","!ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ak0JaJpS0zk9","executionInfo":{"status":"ok","timestamp":1652384044322,"user_tz":-480,"elapsed":195,"user":{"displayName":"Peilin Zhan","userId":"07435443963307107662"}},"outputId":"1edf8b9b-91b1-4ddd-f39a-3e74d2b49764"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":[" 0-.png   4-.png   8-.png       Biometrics.ipynb   DL论文\n"," 1-.png   5-.png   9-.png      'Colab Notebooks'   Sketch.ipynb\n"," 2-.png   6-.png   abc.png      data\t\t   Untitled0.ipynb\n"," 3-.png   7-.png   biometrics  'DL Lab'\n"]}]},{"cell_type":"code","source":["train = MNIST('./data', train=True, download=True, transform=transforms.Compose([\n","    transforms.ToTensor(), # ToTensor does min-max normalization. \n","]), )\n","\n","test = MNIST('./data', train=False, download=True, transform=transforms.Compose([\n","    transforms.ToTensor(), # ToTensor does min-max normalization. \n","]), )\n","\n","# Create DataLoader\n","dataloader_args = dict(shuffle=True, batch_size=256,num_workers=4, pin_memory=True) if cuda else dict(shuffle=True, batch_size=64)\n","train_loader = dataloader.DataLoader(train, **dataloader_args)\n","test_loader = dataloader.DataLoader(test, **dataloader_args)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2XuDrD74h7n1","executionInfo":{"status":"ok","timestamp":1652384048050,"user_tz":-480,"elapsed":1661,"user":{"displayName":"Peilin Zhan","userId":"07435443963307107662"}},"outputId":"c0d7b5f7-5f79-4572-879e-02f5b83ef45e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]}]},{"cell_type":"code","source":["class Model(nn.Module):\n","    def __init__(self):\n","        super(Model, self).__init__()\n","        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n","        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n","        self.fc1 = nn.Linear(4*4*50, 500)\n","        self.fc2 = nn.Linear(500, 10)\n","\n","    def forward(self, x):\n","        x_1 = F.relu(self.conv1(x))\n","        x = F.max_pool2d(x_1, 2, 2)\n","        x_2 = F.relu(self.conv2(x))\n","        x = F.max_pool2d(x_2, 2, 2)\n","        x = x.view(-1, 4*4*50)\n","        x_3 = F.relu(self.fc1(x))\n","        h = F.softmax(self.fc2(x_3), dim=1)\n","        return h, x_3, x_2, x_1       \n","             \n","      \n","    \n","model = Model()\n","if cuda:\n","    model.cuda() # CUDA!\n","optimizer = optim.Adam(model.parameters(), lr=1e-3)    "],"metadata":{"id":"dqV7e3-SiA74"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["EPOCHS = 15\n","losses = []\n","\n","model.train()\n","for epoch in range(EPOCHS):\n","    for batch_idx, (data, target) in enumerate(train_loader):\n","        # Get Samples\n","        data, target = Variable(data), Variable(target)\n","        \n","        if cuda:\n","            data, target = data.cuda(), target.cuda()\n","        \n","        # Init\n","        optimizer.zero_grad()\n","\n","        # Predict\n","        y_pred, _, _, _ = model(data) \n","\n","        # Calculate loss\n","        loss = F.cross_entropy(y_pred, target)\n","        losses.append(loss.cpu().data)\n","#         losses.append(loss.cpu().data[0])        \n","        # Backpropagation\n","        loss.backward()\n","        optimizer.step()\n","        \n","        \n","        # Display\n","        if batch_idx % 100 == 1:\n","            print('\\r Train Epoch: {}/{} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n","                epoch+1,\n","                EPOCHS,\n","                batch_idx * len(data), \n","                len(train_loader.dataset),\n","                100. * batch_idx / len(train_loader), \n","                loss.cpu().data), \n","                end='')\n","    # Eval\n","    evaluate_x = Variable(test_loader.dataset.test_data.type_as(torch.FloatTensor()))\n","    evaluate_y = Variable(test_loader.dataset.test_labels)\n","    if cuda:\n","        evaluate_x, evaluate_y = evaluate_x.cuda(), evaluate_y.cuda()\n","\n","    model.eval()\n","    output, _, _, _ = model(evaluate_x[:,None,...])\n","    pred = output.data.max(1)[1]\n","    d = pred.eq(evaluate_y.data).cpu()\n","    accuracy = d.sum().type(dtype=torch.float64)/d.size()[0]\n","    \n","    print('\\r Train Epoch: {}/{} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\t Test Accuracy: {:.4f}%'.format(\n","        epoch+1,\n","        EPOCHS,\n","        len(train_loader.dataset), \n","        len(train_loader.dataset),\n","        100. * batch_idx / len(train_loader), \n","        loss.cpu().data,\n","        accuracy*100,\n","        end=''))"],"metadata":{"id":"yH17eIMxlCmb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# based on confidence\n","\n","batch_size = 10000\n","all_size = 100000\n","iters = 100\n","\n","stats = dict()\n","for i in range(10):\n","    stats[i] = 0    \n","    \n","avgs = torch.zeros(iters, 10, 28*28)\n","\n","for kk in range(iters):\n","  print(kk)\n","  \n","  z = torch.rand(all_size, 1, 28, 28) #.cuda()  \n","  z.cuda()\n","\n","  all_preds = []\n","  all_confs = []\n","  all_idx = torch.ones(all_size, dtype = torch.uint8)\n","\n","  for k in range(0,all_size, batch_size):\n","      y_pred, _, _, _ = model(z[k:k+batch_size].cuda())\n","      \n","      # erasing the low confident ones!\n","      y_pred[y_pred <= 0.3]= 0\n","      indices = torch.ones(y_pred.size(0), dtype = torch.uint8)\n","      # indices[torch.mean(y_pred, dim =1)==0] = 0 \n","      all_idx[k:k+batch_size] = indices \n","      conf, pred = y_pred[indices==1].data.max(1)#[1]\n","\n","      #if (not torch.isnan(conf).sum()) & (not torch.isnan(pred).sum())  :\n","      all_preds.append(pred)\n","      all_confs.append(conf)\n","  \n","  all_preds = torch.cat(all_preds)\n","  all_confs = torch.cat(all_confs)    \n","  \n","\n","\n","# erasing the low confident ones!\n","  z = z[all_idx]\n","    \n","  for i in range(10):\n","    stats[i] += torch.sum(all_preds==i)\n","    a = torch.mean(z[all_preds==i] , dim=0) \n","#     print(z[preds==i].shape)\n","    avgs[kk, i] = a.reshape(28*28)\n","\n","  \n","  \n","  \n","  \n","  \n","print(stats)  \n","  \n","  \n","  \n","# plotting  \n","save_path = '/content/gdrive/My Drive'\n","\n","\n","# setting NANs to 0\n","avgs[avgs != avgs] = 0 \n","\n","dd = torch.mean(avgs, dim=0)\n","\n","#dd = dd - grand_mean\n","for kk in range(10):\n","  \n","  fig = plt.figure()\n","  a = dd[kk]\n","  a = a.view(-1,28)\n","  b,_,_,_ = model(a[None,None,...].cuda())\n","\n","  #a = torch.nn.functional.log_softmax(a)# torch.nn.functional.softmax(a)\n","  conf, c = b.data.max(1) #[1]\n","  plt.title(f'gt: {str(kk)}  -  pred: {str(c.cpu().data[0].numpy())} - conf: {str(conf.cpu().data[0].numpy())} -  size-gt: {stats[kk]}')  \n","  plt.imshow(a) #, cmap = 'gray')\n","  fig.savefig(os.path.join(save_path, str(kk)+'-.png'))\n","\n"],"metadata":{"id":"WrIro5Uu66QS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[" \n","\n","# Load data\n","# data = mnist_data()\n","\n","# tt = data.targets[data.targets==9]\n","# dd = data.data[data.targets==9] \n","# data.targets = tt\n","# data.data = dd\n","# data_loader = torch.utils.data.DataLoader(data, batch_size=100, shuffle=True, drop_last =True)\n","# # Num batches\n","\n","# z = next(iter(test_loader)) \n","z = test.data # 10K test images\n","z = z[:,None,...]\n","z = z.type(torch.FloatTensor)\n","z = (z - z.min())/(z.max() - z.min())\n","\n","\n","\n","# noise = torch.rand(1, 1, 28, 28) #.cuda()\n","weight = .5\n","\n","\n","# frequency of noise predictions\n","f, axarr = plt.subplots(11, 3)\n","# f.set_figheight(1.4)\n","# f.set_figwidth(15)\n","f.set_figheight(15)\n","f.set_figwidth(15)\n","\n","\n","\n","\n","# fixed noise \n","axarr[0,0].imshow(torch.torch.ones_like(torch.squeeze(z[0,...],0)))\n","axarr[0,0].axis('off')\n","\n","\n","# fixed noise \n","axarr[0,1].imshow(torch.squeeze(z[0,...],0))\n","axarr[0,1].axis('off')\n","\n","\n","\n","y_pred,_ ,_ ,_ = model(z.cuda())\n","preds = y_pred.data.max(1)[1].cpu()\n","# print(preds)\n","freq = np.histogram(preds.numpy(), bins=list(range(11))) \n","y_pos = np.arange(len(freq[0]))\n","axarr[0,2].bar(y_pos, freq[0]/freq[0].sum())\n","axarr[0,2].set_yticks([0.5,1]) \n","\n","\n","mis_class = []\n","\n","\n","for i in range(1,11):\n","  # pattern\n","  pattern =  io.imread(os.path.join(save_path, str(i-1) + '-.png'))\n","  pattern = pattern[35:35+217,113:330,:]\n","  pattern = transform.resize(pattern, (28, 28))\n","  pattern = torch.from_numpy(pattern)\n","  pattern = pattern.type(torch.FloatTensor)\n","  pattern = torch.mean(pattern, dim=2)\n","\n","  axarr[i,0].imshow(pattern)\n","  axarr[i,0].axis('off')\n","  \n","  \n","  # pattern + noise\n","  z_new = (1-weight)*z + weight*pattern #/ (weight+1)  # noise + stim\n","#   z_new = (z_new - z_new.min()) / (z_new.min() - z_new.min())\n","#   print(z_new.min(),z_new.max())\n","  axarr[i,1].imshow(torch.squeeze(z_new[0,...],0))\n","  axarr[i,1].axis('off')\n","  \n","  \n","  # frequency of pattern + noise predictions  \n","  y_pred,_ ,_ ,_ = model(z_new.cuda())\n","  preds = y_pred.data.max(1)[1].cpu()\n","#   print(preds)  \n","  freq = np.histogram(preds.numpy(), bins=list(range(11))) \n","  y_pos = np.arange(len(freq[0]))\n","  axarr[i,2].bar(y_pos, freq[0]/freq[0].sum())\n","  axarr[i,2].set_xticks(list(range(10))) \n","  axarr[i,2].set_yticks([0.5,1]) \n","  preds \n","  \n","  f.subplots_adjust(hspace=0.06) #, wspace=0.0, right = 0.8)\n","  f.show()\n","  \n","  \n","  # targetted attack!\n","  error = (preds == i-1).sum().type(torch.FloatTensor)/preds.size(0)\n","  # untargetted attack!\n","  # error = (preds != test.targets).sum().type(torch.FloatTensor)/preds.size(0)\n","\n","  print(f'misclassification rate: {error}')\n","  mis_class.append(error)\n","  \n","print(np.mean(mis_class))"],"metadata":{"id":"bE5M2Gbo_apI"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BMWCoR4Z6LgS"},"outputs":[],"source":["# load the zero image\n","\n","\n","# Load data\n","\n","# # Num batches\n","\n","z = test.data # 10K test images\n","z = z[:,None,...]\n","z = z.type(torch.FloatTensor)\n","z = (z - z.min())/(z.max() - z.min())\n","\n","\n","\n","# noise = torch.rand(1, 1, 28, 28) #.cuda()\n","weight = .9\n","\n","\n","# frequency of noise predictions\n","f, axarr = plt.subplots(11, 3)\n","# f.set_figheight(1.4)\n","# f.set_figwidth(15)\n","f.set_figheight(15)\n","f.set_figwidth(15)\n","\n","\n","\n","\n","# fixed noise \n","axarr[0,0].imshow(torch.torch.ones_like(torch.squeeze(z[0,...],0)))\n","axarr[0,0].axis('off')\n","\n","\n","# fixed noise \n","axarr[0,1].imshow(torch.squeeze(z[0,...],0))\n","axarr[0,1].axis('off')\n","\n","\n","\n","y_pred, _, _, _= model(z.cuda())\n","preds = y_pred.data.max(1)[1].cpu()\n","# print(preds)\n","freq = np.histogram(preds.numpy(), bins=list(range(11))) \n","y_pos = np.arange(len(freq[0]))\n","axarr[0,2].bar(y_pos, freq[0]/freq[0].sum())\n","axarr[0,2].set_yticks([0.5,1]) \n","\n","\n","mis_class = []\n","\n","\n","for i in range(1,11):\n","  # pattern\n","  pattern = io.imread(os.path.join(save_path, str(i-1) + '-.png'))\n","  pattern = pattern[35:35+217,113:330,:]\n","  pattern = transform.resize(pattern, (28, 28))\n","  pattern = torch.from_numpy(pattern)\n","  pattern = pattern.type(torch.FloatTensor)\n","  pattern = torch.mean(pattern, dim=2)\n","  # pattern = np.randn(())\n","\n","  axarr[i,0].imshow(pattern)\n","  axarr[i,0].axis('off')\n","  \n","  \n","  # pattern + noise\n","  z_new = (1-weight)*z + weight*pattern #/ (weight+1)  # noise + stim\n","#   z_new = (z_new - z_new.min()) / (z_new.min() - z_new.min())\n","#   print(z_new.min(),z_new.max())\n","  axarr[i,1].imshow(torch.squeeze(z_new[0,...],0))\n","  axarr[i,1].axis('off')\n","  \n","  \n","  # frequency of pattern + noise predictions  \n","  y_pred,_,_,_ = model(z_new.cuda())\n","  preds = y_pred.data.max(1)[1].cpu()\n","#   print(preds)  \n","  freq = np.histogram(preds.numpy(), bins=list(range(11))) \n","  y_pos = np.arange(len(freq[0]))\n","  axarr[i,2].bar(y_pos, freq[0]/freq[0].sum())\n","  axarr[i,2].set_xticks(list(range(10))) \n","  axarr[i,2].set_yticks([0.5,1]) \n","  preds \n","  \n","  f.subplots_adjust(hspace=0.06)\n","  f.show()\n","  \n","  \n","  error = (preds != test.targets).sum().type(torch.FloatTensor)/preds.size(0)\n","  print(f'misclassification rate: {error}')\n","  mis_class.append(error)\n","  \n","print(np.mean(mis_class))"]},{"cell_type":"code","source":["# load the zero image\n","# objects = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n","# save_path = 'drive/My Drive/classification_images/10way-weighted'\n","\n","\n","z = torch.rand(10000, 1, 28, 28) #.cuda()\n","weight = .9\n","\n","\n","# frequency of noise predictions\n","f, axarr = plt.subplots(11, 3)\n","# f.set_figheight(1.4)\n","# f.set_figwidth(15)\n","f.set_figheight(15)\n","f.set_figwidth(15)\n","\n","\n","\n","\n","# fixed noise \n","axarr[0,0].imshow(torch.torch.ones_like(torch.squeeze(z[0,...],0)))\n","axarr[0,0].axis('off')\n","\n","\n","# fixed noise \n","axarr[0,1].imshow(torch.squeeze(z[0,...],0))\n","axarr[0,1].axis('off')\n","\n","\n","\n","y_pred,_ , _, _ = model(z.cuda())\n","preds = y_pred.data.max(1)[1].cpu()\n","# print(preds)\n","freq = np.histogram(preds.numpy(), bins=list(range(11))) \n","y_pos = np.arange(len(freq[0]))\n","axarr[0,2].bar(y_pos, freq[0]/freq[0].sum())\n","axarr[0,2].set_yticks([0.5,1]) \n","# axarr[0,2].set_xticks(list(range(9))) \n","# axarr[0,2].set_ylim([0,1])\n","# axarr[0,2].axis('on')\n","\n","\n","\n","# f, axarr = plt.subplots(10, 3, )\n","# f.set_figheight(15)\n","# f.set_figwidth(15)\n","\n","mis_class = []\n","\n","for i in range(1,11):\n","  # pattern\n","  pattern =  io.imread(os.path.join(save_path, str(i-1) + '-.png'))\n","  pattern = pattern[35:35+217,113:330,:]\n","  pattern = transform.resize(pattern, (28, 28))\n","  pattern = torch.from_numpy(pattern)\n","  pattern = pattern.type(torch.FloatTensor)\n","  pattern = torch.mean(pattern, dim=2)\n","\n","  axarr[i,0].imshow(pattern)\n","  axarr[i,0].axis('off')\n","  \n","  \n","\n","  \n","  # pattern + noise\n","  z_new = (1-weight)*z + weight*pattern #/ (weight+1)  # noise + stim\n","  # z_new = (z_new - z_new.min()) / (z_new.min() - z_new.min())\n","#   print(z_new.min(),z_new.max())\n","  axarr[i,1].imshow(torch.squeeze(z_new[0,...],0))\n","  axarr[i,1].axis('off')\n","\n","  \n","  \n","  \n","  # frequency of pattern + noise predictions  \n","  y_pred,_,_,_ = model(z_new.cuda())\n","  preds = y_pred.data.max(1)[1].cpu()\n","#   print(preds)  \n","  freq = np.histogram(preds.numpy(), bins=list(range(11))) \n","  y_pos = np.arange(len(freq[0]))\n","  axarr[i,2].bar(y_pos, freq[0]/freq[0].sum())\n","  axarr[i,2].set_xticks(list(range(10))) \n","  axarr[i,2].set_yticks([0.5,1]) \n","\n","  \n","  f.subplots_adjust(hspace=0.06) #, wspace=0.0, right = 0.8)\n","  f.show()\n","  \n","#   axarr[i,2].axis('off')\n","\n","  error = (preds != i-1).sum().type(torch.FloatTensor)/preds.size(0)\n","  print(f'misclassification rate: {error}')\n","  mis_class.append(error)\n","  \n","print(np.mean(mis_class))"],"metadata":{"id":"DkFsb2d12-il"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Create mean images\n","# computing the average mnist images\n","f, axarr = plt.subplots(2, 5)\n","# f.set_figheight(1.4)\n","# f.set_figwidth(15)\n","f.set_figheight(5)\n","f.set_figwidth(15)\n","f.subplots_adjust(hspace=0.36) #, wspace=0.0, right = 0.8)\n","\n","\n","mean_imgs = []\n","for i in range(10):\n","  plt.figure()\n","  m = torch.mean(train.data[train.targets == i].type(torch.FloatTensor), dim=0)\n","  mean_imgs.append(m)\n","  axarr[i//5,i%5].imshow(m) \n","  "],"metadata":{"id":"ydOdu8tI8S2Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(len(mean_imgs))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aXEDkN7--FSA","executionInfo":{"status":"ok","timestamp":1652370746046,"user_tz":-480,"elapsed":216,"user":{"displayName":"Peilin Zhan","userId":"07435443963307107662"}},"outputId":"a3b12a64-ebab-4876-ee80-a7ad5ef2cafb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["10\n"]}]},{"cell_type":"code","source":["#mean images\n","\n","\n","z = test.data # 10K test images\n","z = z[:,None,...]\n","z = z.type(torch.FloatTensor)\n","z = (z - z.min())/(z.max() - z.min())\n","\n","weight = .9\n","\n","\n","# frequency of noise predictions\n","f, axarr = plt.subplots(11, 3)\n","# f.set_figheight(1.4)\n","# f.set_figwidth(15)\n","f.set_figheight(15)\n","f.set_figwidth(15)\n","\n","save_path = '/content/gdrive/My Drive'\n","\n","\n","# fixed noise \n","axarr[0,0].imshow(torch.torch.ones_like(torch.squeeze(z[0,...],0)))\n","axarr[0,0].axis('off')\n","\n","\n","# fixed noise \n","axarr[0,1].imshow(torch.squeeze(z[0,...],0))\n","axarr[0,1].axis('off')\n","\n","\n","\n","y_pred,_ , _, _ = model(z.cuda())\n","preds = y_pred.data.max(1)[1].cpu()\n","# print(preds)\n","freq = np.histogram(preds.numpy(), bins=list(range(11))) \n","y_pos = np.arange(len(freq[0]))\n","axarr[0,2].bar(y_pos, freq[0]/freq[0].sum())\n","axarr[0,2].set_yticks([0.5,1]) \n","\n","\n","\n","\n","# f, axarr = plt.subplots(10, 3, )\n","# f.set_figheight(15)\n","# f.set_figwidth(15)\n","\n","mis_class = []\n","\n","for i in range(1,11):\n","  # pattern\n","  pattern = mean_imgs[i-1]\n","  pattern = (pattern - pattern.min()) / (pattern.max() - pattern.min())\n","\n","  axarr[i,0].imshow(pattern)\n","  axarr[i,0].axis('off')\n","  \n","  \n","\n","  \n","  # pattern + noise\n","  z_new = (1-weight)*z + weight*pattern #/ (weight+1)  # noise + stim\n","#   # z_new = (z_new - z_new.min()) / (z_new.min() - z_new.min())\n","# #   print(z_new.min(),z_new.max())\n","  axarr[i,1].imshow(torch.squeeze(z_new[0,...],0))\n","  axarr[i,1].axis('off')\n","  \n","  \n","  \n","  \n","  # frequency of pattern + noise predictions  \n","  y_pred, _, _, _ = model(z_new.cuda())\n","  preds = y_pred.data.max(1)[1].cpu()\n","#   print(preds)  \n","  freq = np.histogram(preds.numpy(), bins=list(range(11))) \n","  y_pos = np.arange(len(freq[0]))\n","  axarr[i,2].bar(y_pos, freq[0]/freq[0].sum())\n","  axarr[i,2].set_xticks(list(range(10))) \n","  axarr[i,2].set_yticks([0.5,1]) \n","\n","  \n","  f.subplots_adjust(hspace=0.06) #, wspace=0.0, right = 0.8)\n","  f.show()\n","  \n","#   axarr[i,2].axis('off')\n","\n","  error = (preds != i-1).sum().type(torch.FloatTensor)/preds.size(0)\n","  print(f'misclassification rate: {error}')\n","  mis_class.append(error)\n","  \n","print(np.mean(mis_class))"],"metadata":{"id":"RCVKUVEc3CPt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import confusion_matrix\n","from sklearn.utils.multiclass import unique_labels\n","import copy\n","import matplotlib.cm as cm\n","\n","x = np.arange(11)\n","ys = [i+x+(i*x)**2 for i in range(11)]\n","colors = cm.rainbow(np.linspace(0, 1, len(ys)))\n","# for y, c in zip(ys, colors):\n","#     plt.scatter(x, y, color=c)\n","\n","\n","\n","\n","\n","# - measure response to a noise pattern\n","# - increase the bias for all neurons proportional to their response in a certain layer\n","# - see if the classification has been biased toward the class of favor\n","\n","test_x = test.data.type(torch.FloatTensor).clone() # Variable(test_loader.dataset.test_data.type_as(torch.FloatTensor()))\n","test_y = test.targets.type(torch.ByteTensor).clone() #Variable(test_loader.dataset.test_labels)\n","# test_x = test_x/256\n","\n","train_x = train.data.type(torch.FloatTensor).clone() # Variable(test_loader.dataset.test_data.type_as(torch.FloatTensor()))\n","train_y = train.targets.type(torch.ByteTensor).clone() #Variable(test_loader.dataset.test_labels)\n","\n","\n","f, axarr = plt.subplots(3, 5, )\n","f.set_figheight(20)\n","f.set_figwidth(30)\n","\n","\n","f2, axarr2 = plt.subplots(1, 21, )\n","f2.set_figheight(5)\n","f2.set_figwidth(30)\n","\n","all_size = test_x.size(0)\n","\n","layers = ['fc1', 'conv2', 'conv1']\n","\n","\n","dig_names = [str(i) for i in range(5,10)]\n","\n","for l_idx, layer in enumerate(layers):\n","\n","  for which in range(5,10):\n","    which_data = train_x[train_y == which]/255.    # 1 vs. 7\n","\n","    with torch.no_grad():\n","\n","      for k_idx, k in enumerate(range(0,11)):\n","        fraction_pred_favor = []\n","\n","        # modulate model\n","        model2 = Model()\n","        model2.load_state_dict(copy.deepcopy(model.state_dict()))\n","        model2.cuda()\n","        model2.eval()\n","        y_pred_x, fc1, conv2, conv1 = model2(which_data[:,None,...].cuda()) # amplify zero sensitive ones\n","        \n","        if layer == 'conv1':\n","          avg_activation = torch.mean(conv1, dim=0).mean(-1).mean(-1)\n","          aa = avg_activation/avg_activation.max()          \n","          model2.conv1.bias -= 0.1*k*aa # * model2.fc1.bias  #*avg_activation\n","#           l_legend = [str(k*i) for i in range(0,11)]          \n","          l_legend = ['{0:.0f}'.format(0.1*k*i) for i in range(0,11)]\n","        elif  layer == 'conv2':\n","          avg_activation = torch.mean(conv2, dim=0).mean(-1).mean(-1)\n","          aa = avg_activation/avg_activation.max()          \n","          model2.conv2.bias -= k*aa # * model2.fc1.bias  #*avg_activation\n","          l_legend = [str(k*i) for i in range(0,11)]          \n","        else:\n","          avg_activation = torch.mean(fc1, dim=0)\n","          aa = avg_activation/avg_activation.max()\n","          model2.fc1.bias -= 0.01*k* avg_activation  #*avg_activation\n","          l_legend = ['{0:.1f}'.format(k*0.01*i) for i in range(0,11)]\n","\n","        for idx, weight in enumerate(np.linspace(0,1,21)):    \n","            uu = test_x[test_y==which].clone()/255.\n","            z = torch.rand(uu.size(0), 28, 28) #.cuda()  \n","            z.cuda()\n","\n","    #         stimuli = (1*z + weight*uu)/ (weight+1)\n","            stimuli = (1-weight)*z + weight*uu\n","#             stimuli = uu\n","\n","            output, _, _, _ = model(stimuli[:,None,...].cuda())\n","            pred_n = output.data.max(1)[1]\n","            pred_n = pred_n.type(torch.ByteTensor).squeeze()\n","\n","            if k==0:  \n","              axarr2[idx].imshow(stimuli[0])\n","              axarr2[idx].axis('off')      \n","              axarr2[idx].set_title('{:.2f}'.format((weight)))\n","\n","            output_m, _, _, _ = model2(stimuli[:,None,...].cuda())\n","            pred_m = output_m.data.max(1)[1]\n","            pred_m = pred_m.type(torch.ByteTensor).squeeze()\n","\n","            \n","            pp = (pred_n == which).sum().type(torch.FloatTensor) / pred_n.size(0) #       (test_y == which).sum()\n","            qq = (pred_m == which).sum().type(torch.FloatTensor)  / pred_m.size(0) #       (test_y == which).sum()  \n","            fraction_pred_favor.append([pp, qq])\n","\n","        mat = np.array(fraction_pred_favor)\n","#         if k_idx == 0:\n","#           axarr[which].plot(mat[:,0], 'b')\n","        axarr[l_idx, which-5].plot(mat[:,1], color=(colors[k_idx,:3]))\n","        axarr[l_idx, which-5].set_yticks([0.5,1])     \n","#         if which==0:\n","        axarr[l_idx, which-5].set_ylabel('acc')\n","        axarr[l_idx, which-5].set_title(dig_names[which-5] + '-' + layer)\n","  \n","        axarr[l_idx, which-5].set_xticks([i for i in range(0,21,4)])       \n","        axarr[l_idx, which-5].set_xticklabels(['{:.1f}'.format(i/20) for i in range(0,21,4)])\n","        l_legend = ['- ' + q for q in l_legend]\n","        axarr[l_idx, which-5].legend(l_legend)                   \n","\n","f.subplots_adjust(hspace=0.8) #, wspace=0.0, right = 0.8)\n","f.show()"],"metadata":{"id":"D3cmVwQb9C2a"},"execution_count":null,"outputs":[]}]}