{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Filter Visualization.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPfTvQ1fLj2IS8yKKTguTo/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9PZNca78Bo-g","executionInfo":{"status":"ok","timestamp":1652373847726,"user_tz":-480,"elapsed":216,"user":{"displayName":"Peilin Zhan","userId":"07435443963307107662"}},"outputId":"dd205e81-8bdd-44e9-a59b-5090602eed8d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Populating the interactive namespace from numpy and matplotlib\n"]}],"source":["%pylab inline\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.utils.data.dataloader as dataloader\n","import torch.optim as optim\n","\n","from torch.utils.data import TensorDataset\n","from torch.autograd import Variable\n","from torchvision import transforms\n","from torchvision.datasets import MNIST\n","from tqdm import tqdm\n","from time import sleep\n","import pickle\n","import matplotlib.pyplot as plt\n","import math\n","import sys\n","\n","\n","SEED = 1\n","\n","# CUDA?\n","cuda = torch.cuda.is_available()\n","\n","# For reproducibility\n","torch.manual_seed(SEED)\n","\n","if cuda:\n","    torch.cuda.manual_seed(SEED)"]},{"cell_type":"code","source":["\n","train = MNIST('./data', train=True, download=True, transform=transforms.Compose([\n","    transforms.ToTensor(), # ToTensor does min-max normalization. \n","]), )\n","\n","test = MNIST('./data', train=False, download=True, transform=transforms.Compose([\n","    transforms.ToTensor(), # ToTensor does min-max normalization. \n","]), )\n","\n","# Create DataLoader\n","dataloader_args = dict(shuffle=True, batch_size=256,num_workers=4, pin_memory=True) if cuda else dict(shuffle=True, batch_size=64)\n","train_loader = dataloader.DataLoader(train, **dataloader_args)\n","test_loader = dataloader.DataLoader(test, **dataloader_args)"],"metadata":{"id":"KLd3i07bCfsI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Model(nn.Module):\n","    def __init__(self):\n","        super(Model, self).__init__()\n","        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n","        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n","        self.fc1 = nn.Linear(4*4*50, 500)\n","        self.fc2 = nn.Linear(500, 10)\n","\n","    def forward(self, x):\n","        x1 = F.relu(self.conv1(x))\n","        x = F.max_pool2d(x1, 2, 2)\n","        x2 = F.relu(self.conv2(x))\n","        x = F.max_pool2d(x2, 2, 2)\n","        x = x.view(-1, 4*4*50)\n","        x = F.relu(self.fc1(x))\n","        x = self.fc2(x)\n","        return x, x2, x1\n","    \n","model = Model()\n","if cuda:\n","    model.cuda() # CUDA!\n","optimizer = optim.Adam(model.parameters(), lr=1e-3) "],"metadata":{"id":"8tD9AujUChVc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["EPOCHS = 15\n","losses = []\n","\n","model.train()\n","best_acc = 0\n","for epoch in range(EPOCHS):\n","    for batch_idx, (data, target) in enumerate(train_loader):\n","        # Get Samples\n","        data, target = Variable(data), Variable(target)\n","        \n","        if cuda:\n","            data, target = data.cuda(), target.cuda()\n","        \n","        # Init\n","        optimizer.zero_grad()\n","\n","        # Predict\n","        y_pred = model(data)[0]\n","\n","        # Calculate loss\n","        loss = F.cross_entropy(y_pred, target)\n","        losses.append(loss.cpu().data)       \n","        # Backpropagation\n","        loss.backward()\n","        optimizer.step()\n","        \n","        \n","        # Display\n","        if batch_idx % 100 == 1:\n","            print('\\r Train Epoch: {}/{} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n","                epoch+1,\n","                EPOCHS,\n","                batch_idx * len(data), \n","                len(train_loader.dataset),\n","                100. * batch_idx / len(train_loader), \n","                loss.cpu().data), \n","                end='')\n","    # Eval\n","    evaluate_x = Variable(test_loader.dataset.data.type_as(torch.FloatTensor()))\n","    evaluate_y = Variable(test_loader.dataset.targets)\n","    if cuda:\n","        evaluate_x, evaluate_y = evaluate_x.cuda(), evaluate_y.cuda()\n","\n","    model.eval()\n","    output = model(evaluate_x[:,None,...])[0]\n","    pred = output.data.max(1)[1]\n","    d = pred.eq(evaluate_y.data).cpu()\n","    accuracy = d.sum().type(dtype=torch.float64)/d.size()[0]\n","    \n","    # save best\n","    if accuracy > best_acc:\n","        best_acc = accuracy\n","        torch.save({\n","                  'model': model.state_dict(),\n","                 }, \"./bestcnn.weights\")\n","        print('\\r Best model saved.\\r')\n","        \n","    print('\\r Train Epoch: {}/{} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\t Test Accuracy: {:.4f}%'.format(\n","        epoch+1,\n","        EPOCHS,\n","        len(train_loader.dataset), \n","        len(train_loader.dataset),\n","        100. * batch_idx / len(train_loader), \n","        loss.cpu().data,\n","        accuracy*100,\n","        end=''))"],"metadata":{"id":"6r-McyCqGgAc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["weights = model.conv1.weight.data.cpu()\n","\n","# plot the first layer features\n","for i in range(weights.shape[0]):\n","    plt.subplot(4, 5, i+1)\n","    plt.axis('off')\n","    to_show = weights[i][0].cpu().numpy()\n","    plt.imshow((to_show - to_show.min())/(to_show.max() - to_show.min()))\n","plt.show()\n","\n"],"metadata":{"id":"apZbo4UPJ1hS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["weights = model.conv2.weight.data.cpu()\n","print(weights.shape)\n","for i in range(weights.shape[1]):\n","    plt.subplot(5,4, i+1)\n","    plt.axis('off')\n","    to_show = weights[i][0].cpu().numpy()\n","    # plt.imshow(rf['conv2_rf'][i].numpy()/(rf['processed']))\n","    plt.imshow((to_show - to_show.min())/(to_show.max() - to_show.min()))\n","plt.show()"],"metadata":{"id":"YyOFzyAbMhc3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["convnet =   [[5,1,0], [2,2,0], [5,1,0], [2,2,0]]\n","layer_names = ['conv1','pool1','conv2','pool2']\n","imsize = 28\n","\n","def outFromIn(conv, layerIn):\n","    n_in = layerIn[0]\n","    j_in = layerIn[1]\n","    r_in = layerIn[2]\n","    start_in = layerIn[3]\n","    \n","    k = conv[0]\n","    s = conv[1]\n","    p = conv[2]\n","    \n","    n_out = math.floor((n_in - k + 2*p)/s) + 1\n","    actualP = (n_out-1)*s - n_in + k\n","    pR = math.ceil(actualP/2)\n","    pL = math.floor(actualP/2)\n","    \n","    j_out = j_in * s\n","    r_out = r_in + (k - 1)*j_in\n","    start_out = start_in + ((k-1)/2 - pL)*j_in\n","    return n_out, j_out, r_out, start_out\n","  \n","def printLayer(layer, layer_name):\n","    print(layer_name + \":\")\n","    print(\"\\t n features: %s \\n \\t jump: %s \\n \\t receptive size: %s \\t start: %s \" % (\n","        layer[0], layer[1], layer[2], layer[3]))\n","    \n","layerInfos = []\n","#first layer is the data layer (image) with n_0 = image size; j_0 = 1; r_0 = 1; and start_0 = 0.5\n","print (\"-------Net summary------\")\n","currentLayer = [imsize, 1, 1, 0.5]\n","printLayer(currentLayer, \"input image\")\n","for i in range(len(convnet)):\n","    currentLayer = outFromIn(convnet[i], currentLayer)\n","    layerInfos.append(currentLayer)\n","    printLayer(currentLayer, layer_names[i])\n","print (\"------------------------\")"],"metadata":{"id":"NxuK7czYL89E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# generate random pattern, recording conv1 and conv2 neuron responses\n","iter_ = 100\n","all_size = 10000\n","p = 28\n","\n","pos_x = 5\n","pos_y = 5\n","\n","model.eval()\n","act_conv1 = []\n","act_conv2 = []\n","noise = []\n","with tqdm(total=iter_, file=sys.stdout) as pbar:\n","    for i in range(iter_):\n","        z = torch.rand(all_size, p, p)\n","        if cuda:\n","            z = z.cuda()\n","        with torch.no_grad():\n","            _, x2, x1 = model(z[:,None,...])\n","        act_conv2.append(x2[:, :, pos_x, pos_y].cpu())\n","        act_conv1.append(x1[:, :, pos_x, pos_y].cpu())\n","        noise.append(z.cpu())\n","\n","        pbar.update(1)\n","\n","act_conv1 = torch.cat(act_conv1)\n","act_conv2 = torch.cat(act_conv2)\n","noise = torch.cat(noise)\n","\n","print(act_conv1.shape)\n","print(act_conv2.shape)\n","print(noise.shape)\n","print(act_conv1)\n","print(act_conv2)"],"metadata":{"id":"XIAodE91Ohh2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import confusion_matrix\n","from sklearn.utils.multiclass import unique_labels\n","import copy\n","import matplotlib.cm as cm\n","\n","x = np.arange(11)\n","ys = [i+x+(i*x)**2 for i in range(11)]\n","colors = cm.rainbow(np.linspace(0, 1, len(ys)))\n","\n","# - measure response to a noise pattern\n","# - increase the bias for all neurons proportional to their response in a certain layer\n","# - see if the classification has been biased toward the class of favor\n","\n","test_x = test.data.type(torch.FloatTensor).clone() # Variable(test_loader.dataset.test_data.type_as(torch.FloatTensor()))\n","test_y = test.targets.type(torch.ByteTensor).clone() #Variable(test_loader.dataset.test_labels)\n","# test_x = test_x/256\n","\n","train_x = train.data.type(torch.FloatTensor).clone() # Variable(test_loader.dataset.test_data.type_as(torch.FloatTensor()))\n","train_y = train.targets.type(torch.ByteTensor).clone() #Variable(test_loader.dataset.test_labels)\n","\n","\n","f, axarr = plt.subplots(3, 5, )\n","f.set_figheight(20)\n","f.set_figwidth(30)\n","\n","\n","f2, axarr2 = plt.subplots(1, 21, )\n","f2.set_figheight(5)\n","f2.set_figwidth(30)\n","\n","all_size = test_x.size(0)\n","\n","layers = ['fc1', 'conv2', 'conv1']\n","\n","\n","dig_names = [str(i) for i in range(5,10)]\n","\n","for l_idx, layer in enumerate(layers):\n","\n","  for which in range(5,10):\n","    which_data = train_x[train_y == which]/255.    # 1 vs. 7\n","\n","    with torch.no_grad():\n","\n","      for k_idx, k in enumerate(range(0,11)):\n","        fraction_pred_favor = []\n","\n","        # modulate model\n","        model2 = Model()\n","        model2.load_state_dict(copy.deepcopy(model.state_dict()))\n","        model2.cuda()\n","        model2.eval()\n","        y_pred_x, fc1, conv2, conv1 = model2(which_data[:,None,...].cuda()) # amplify zero sensitive ones\n","        \n","        if layer == 'conv1':\n","          avg_activation = torch.mean(conv1, dim=0).mean(-1).mean(-1)\n","          aa = avg_activation/avg_activation.max()          \n","          model2.conv1.bias -= 0.1*k*aa # * model2.fc1.bias  #*avg_activation          \n","          l_legend = ['{0:.0f}'.format(0.1*k*i) for i in range(0,11)]\n","        elif  layer == 'conv2':\n","          avg_activation = torch.mean(conv2, dim=0).mean(-1).mean(-1)\n","          aa = avg_activation/avg_activation.max()          \n","          model2.conv2.bias -= k*aa # * model2.fc1.bias  #*avg_activation\n","          l_legend = [str(k*i) for i in range(0,11)]          \n","        else:\n","          avg_activation = torch.mean(fc1, dim=0)\n","          aa = avg_activation/avg_activation.max()\n","          model2.fc1.bias -= 0.01*k* avg_activation  #*avg_activation\n","          l_legend = ['{0:.1f}'.format(k*0.01*i) for i in range(0,11)]\n","\n","        for idx, weight in enumerate(np.linspace(0,1,21)):    \n","            uu = test_x[test_y==which].clone()/255.\n","            z = torch.rand(uu.size(0), 28, 28) #.cuda()  \n","            z.cuda()\n","\n","    #         stimuli = (1*z + weight*uu)/ (weight+1)\n","            stimuli = (1-weight)*z + weight*uu\n","#             stimuli = uu\n","\n","            output, _, _, _ = model(stimuli[:,None,...].cuda())\n","            pred_n = output.data.max(1)[1]\n","            pred_n = pred_n.type(torch.ByteTensor).squeeze()\n","\n","            if k==0:  \n","              axarr2[idx].imshow(stimuli[0])\n","              axarr2[idx].axis('off')      \n","              axarr2[idx].set_title('{:.2f}'.format((weight)))\n","\n","            output_m, _, _, _ = model2(stimuli[:,None,...].cuda())\n","            pred_m = output_m.data.max(1)[1]\n","            pred_m = pred_m.type(torch.ByteTensor).squeeze()\n","\n","            \n","            pp = (pred_n == which).sum().type(torch.FloatTensor) / pred_n.size(0) #       (test_y == which).sum()\n","            qq = (pred_m == which).sum().type(torch.FloatTensor)  / pred_m.size(0) #       (test_y == which).sum()  \n","            fraction_pred_favor.append([pp, qq])\n","\n","        mat = np.array(fraction_pred_favor)\n","\n","        axarr[l_idx, which-5].plot(mat[:,1], color=(colors[k_idx,:3]))\n","        axarr[l_idx, which-5].set_yticks([0.5,1])     \n","\n","        axarr[l_idx, which-5].set_ylabel('acc')\n","        axarr[l_idx, which-5].set_title(dig_names[which-5] + '-' + layer)\n","  \n","        axarr[l_idx, which-5].set_xticks([i for i in range(0,21,4)])       \n","        axarr[l_idx, which-5].set_xticklabels(['{:.1f}'.format(i/20) for i in range(0,21,4)])\n","        l_legend = ['- ' + q for q in l_legend]\n","        axarr[l_idx, which-5].legend(l_legend)                   \n","\n","f.subplots_adjust(hspace=0.8)\n","f.show()"],"metadata":{"id":"JXCGNQU2rV46"},"execution_count":null,"outputs":[]}]}